{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa690a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d55d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from pydub import AudioSegment\n",
    "import sounddevice as sd\n",
    "from PIL import Image, ImageTk \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['animation.ffmpeg_path'] = '/path/to/ffmpeg'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Couldn't find ffmpeg or avconv.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bd4a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\MAchine Learning Project\\Emotion_Detection_Using_Voice\\TESS Toronto emotional speech set data\\TESS Toronto emotional speech set data\\OAF_angry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\AppData\\Local\\Temp\\ipykernel_20500\\1369180971.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(file_path, res_type='kaiser_fast')\n",
      "c:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\MAchine Learning Project\\\\Emotion_Detection_Using_Voice\\\\TESS Toronto emotional speech set data\\\\TESS Toronto emotional speech set data\\\\OAF_angry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLibsndfileError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:175\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m sf\u001b[38;5;241m.\u001b[39mSoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:208\u001b[0m, in \u001b[0;36m__soundfile_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m# Otherwise, create the soundfile object\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context \u001b[38;5;28;01mas\u001b[39;00m sf_desc:\n",
      "File \u001b[1;32mc:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\soundfile.py:1216\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1215\u001b[0m     err \u001b[38;5;241m=\u001b[39m _snd\u001b[38;5;241m.\u001b[39msf_error(file_ptr)\n\u001b[1;32m-> 1216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LibsndfileError(err, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError opening \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode_int \u001b[38;5;241m==\u001b[39m _snd\u001b[38;5;241m.\u001b[39mSFM_WRITE:\n\u001b[0;32m   1218\u001b[0m     \u001b[38;5;66;03m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[39;00m\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;66;03m# when opening a named pipe in SFM_WRITE mode.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;66;03m# See http://github.com/erikd/libsndfile/issues/77.\u001b[39;00m\n",
      "\u001b[1;31mLibsndfileError\u001b[0m: Error opening 'C:\\\\MAchine Learning Project\\\\Emotion_Detection_Using_Voice\\\\TESS Toronto emotional speech set data\\\\TESS Toronto emotional speech set data\\\\OAF_angry': System error.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_path)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Extract features and labels here\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m data\u001b[38;5;241m.\u001b[39mappend(features)\n\u001b[0;32m     22\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(folder_name)  \u001b[38;5;66;03m# Assuming folder name represents labels\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_features\u001b[39m(file_path):\n\u001b[1;32m----> 2\u001b[0m     audio, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkaiser_fast\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(y\u001b[38;5;241m=\u001b[39maudio, sr\u001b[38;5;241m=\u001b[39msr, n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)\u001b[38;5;241m.\u001b[39mT, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:183\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPurePath)):\n\u001b[0;32m    180\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    181\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySoundFile failed. Trying audioread instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    182\u001b[0m     )\n\u001b[1;32m--> 183\u001b[0m     y, sr_native \u001b[38;5;241m=\u001b[39m \u001b[43m__audioread_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\util\\decorators.py:59\u001b[0m, in \u001b[0;36mdeprecated.<locals>.__wrapper\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Warn the user, and then proceed.\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDeprecated as of librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mIt will be removed in librosa version \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# Would be 2, but the decorator adds a level\u001b[39;00m\n\u001b[0;32m     58\u001b[0m )\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:239\u001b[0m, in \u001b[0;36m__audioread_load\u001b[1;34m(path, offset, duration, dtype)\u001b[0m\n\u001b[0;32m    236\u001b[0m     reader \u001b[38;5;241m=\u001b[39m path\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;66;03m# If the input was not an audioread object, try to open it\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43maudioread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m reader \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[0;32m    242\u001b[0m     sr_native \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39msamplerate\n",
      "File \u001b[1;32mc:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\audioread\\__init__.py:127\u001b[0m, in \u001b[0;36maudio_open\u001b[1;34m(path, backends)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m BackendClass \u001b[38;5;129;01min\u001b[39;00m backends:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBackendClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError:\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\audioread\\rawread.py:59\u001b[0m, in \u001b[0;36mRawAudioFile.__init__\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m aifc\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\MAchine Learning Project\\\\Emotion_Detection_Using_Voice\\\\TESS Toronto emotional speech set data\\\\TESS Toronto emotional speech set data\\\\OAF_angry'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_features(file_path):\n",
    "    audio, sr = librosa.load(file_path, res_type='kaiser_fast')\n",
    "    features = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13).T, axis=0)\n",
    "    return features\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Provide the correct directory path here\n",
    "directory_path = r\"C:\\MAchine Learning Project\\Emotion_Detection_Using_Voice\\TESS Toronto emotional speech set data\"\n",
    "\n",
    "\n",
    "for folder_name in os.listdir(directory_path):\n",
    "    folder_path = os.path.join(directory_path, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            print(file_path)\n",
    "            # Extract features and labels here\n",
    "            features = extract_features(file_path)\n",
    "            data.append(features)\n",
    "            labels.append(folder_name)  # Assuming folder name represents labels\n",
    "\n",
    "    \n",
    "        # Process the file (e.g., load data, extract features, etc.)\n",
    "        # Append data and labels accordingly\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9639a80-fd41-4ca3-b46f-0da848f6556e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15476c07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.0787 - loss: 2.8014 - val_accuracy: 0.1375 - val_loss: 2.6448\n",
      "Epoch 2/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0678 - loss: 2.7509 - val_accuracy: 0.1411 - val_loss: 2.5797\n",
      "Epoch 3/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.0930 - loss: 2.6667 - val_accuracy: 0.0875 - val_loss: 2.6668\n",
      "Epoch 4/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1060 - loss: 2.6412 - val_accuracy: 0.1089 - val_loss: 2.5459\n",
      "Epoch 5/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1182 - loss: 2.5994 - val_accuracy: 0.1696 - val_loss: 2.4036\n",
      "Epoch 6/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1357 - loss: 2.5227 - val_accuracy: 0.1875 - val_loss: 2.3128\n",
      "Epoch 7/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1658 - loss: 2.4119 - val_accuracy: 0.3446 - val_loss: 2.1104\n",
      "Epoch 8/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2061 - loss: 2.2758 - val_accuracy: 0.3304 - val_loss: 1.9765\n",
      "Epoch 9/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2289 - loss: 2.1207 - val_accuracy: 0.3750 - val_loss: 1.8404\n",
      "Epoch 10/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2951 - loss: 1.9716 - val_accuracy: 0.3911 - val_loss: 1.6927\n",
      "Epoch 11/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3373 - loss: 1.8508 - val_accuracy: 0.5875 - val_loss: 1.5324\n",
      "Epoch 12/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3406 - loss: 1.7425 - val_accuracy: 0.5893 - val_loss: 1.4431\n",
      "Epoch 13/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3872 - loss: 1.6155 - val_accuracy: 0.5964 - val_loss: 1.3065\n",
      "Epoch 14/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4441 - loss: 1.5027 - val_accuracy: 0.6875 - val_loss: 1.1988\n",
      "Epoch 15/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4776 - loss: 1.4009 - val_accuracy: 0.7125 - val_loss: 1.0829\n",
      "Epoch 16/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5140 - loss: 1.3016 - val_accuracy: 0.7429 - val_loss: 0.9661\n",
      "Epoch 17/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5705 - loss: 1.1891 - val_accuracy: 0.7679 - val_loss: 0.8647\n",
      "Epoch 18/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6053 - loss: 1.1047 - val_accuracy: 0.8125 - val_loss: 0.7462\n",
      "Epoch 19/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6198 - loss: 1.0255 - val_accuracy: 0.8054 - val_loss: 0.6707\n",
      "Epoch 20/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6626 - loss: 0.9158 - val_accuracy: 0.8518 - val_loss: 0.5846\n",
      "Epoch 21/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7040 - loss: 0.8440 - val_accuracy: 0.8554 - val_loss: 0.5386\n",
      "Epoch 22/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7417 - loss: 0.7756 - val_accuracy: 0.8732 - val_loss: 0.4632\n",
      "Epoch 23/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7532 - loss: 0.6894 - val_accuracy: 0.8768 - val_loss: 0.4307\n",
      "Epoch 24/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7721 - loss: 0.6991 - val_accuracy: 0.8857 - val_loss: 0.3879\n",
      "Epoch 25/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7731 - loss: 0.6342 - val_accuracy: 0.8893 - val_loss: 0.3680\n",
      "Epoch 26/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7934 - loss: 0.5949 - val_accuracy: 0.8946 - val_loss: 0.3360\n",
      "Epoch 27/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8090 - loss: 0.5605 - val_accuracy: 0.9036 - val_loss: 0.3288\n",
      "Epoch 28/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8308 - loss: 0.5234 - val_accuracy: 0.8964 - val_loss: 0.3088\n",
      "Epoch 29/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8248 - loss: 0.5195 - val_accuracy: 0.9018 - val_loss: 0.3099\n",
      "Epoch 30/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8351 - loss: 0.4834 - val_accuracy: 0.8929 - val_loss: 0.3100\n",
      "Epoch 31/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8224 - loss: 0.4776 - val_accuracy: 0.9107 - val_loss: 0.2752\n",
      "Epoch 32/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8398 - loss: 0.4603 - val_accuracy: 0.9107 - val_loss: 0.2727\n",
      "Epoch 33/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8387 - loss: 0.4765 - val_accuracy: 0.9143 - val_loss: 0.2526\n",
      "Epoch 34/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8428 - loss: 0.4513 - val_accuracy: 0.9161 - val_loss: 0.2569\n",
      "Epoch 35/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8504 - loss: 0.4421 - val_accuracy: 0.9036 - val_loss: 0.2658\n",
      "Epoch 36/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8575 - loss: 0.4200 - val_accuracy: 0.9107 - val_loss: 0.2543\n",
      "Epoch 37/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8421 - loss: 0.4548 - val_accuracy: 0.8893 - val_loss: 0.2697\n",
      "Epoch 38/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8472 - loss: 0.4153 - val_accuracy: 0.9036 - val_loss: 0.2433\n",
      "Epoch 39/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8510 - loss: 0.4041 - val_accuracy: 0.9089 - val_loss: 0.2636\n",
      "Epoch 40/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8613 - loss: 0.4022 - val_accuracy: 0.9125 - val_loss: 0.2518\n",
      "Epoch 41/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8549 - loss: 0.4036 - val_accuracy: 0.9071 - val_loss: 0.2722\n",
      "Epoch 42/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8558 - loss: 0.3985 - val_accuracy: 0.9250 - val_loss: 0.2459\n",
      "Epoch 43/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8718 - loss: 0.3575 - val_accuracy: 0.9071 - val_loss: 0.2498\n",
      "Epoch 44/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8528 - loss: 0.3775 - val_accuracy: 0.9321 - val_loss: 0.2230\n",
      "Epoch 45/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8656 - loss: 0.3772 - val_accuracy: 0.9161 - val_loss: 0.2396\n",
      "Epoch 46/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8577 - loss: 0.3986 - val_accuracy: 0.9250 - val_loss: 0.2389\n",
      "Epoch 47/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8781 - loss: 0.3505 - val_accuracy: 0.9179 - val_loss: 0.2314\n",
      "Epoch 48/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8920 - loss: 0.3263 - val_accuracy: 0.9250 - val_loss: 0.2253\n",
      "Epoch 49/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8637 - loss: 0.3706 - val_accuracy: 0.9089 - val_loss: 0.2262\n",
      "Epoch 50/50\n",
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8737 - loss: 0.3581 - val_accuracy: 0.9214 - val_loss: 0.2131\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9015 - loss: 0.2377 \n",
      "Test loss: 0.21306654810905457\n",
      "Test accuracy: 0.9214285612106323\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, TimeDistributed\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check if data and labels_encoded have enough samples\n",
    "if len(data) == 0 or len(labels) == 0:\n",
    "    print(\"Error: Data or labels_encoded is empty.\")\n",
    "else:\n",
    "    # Assuming you have data and labels_encoded defined earlier\n",
    "    label_encoder = LabelEncoder()\n",
    "    labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convert data to numpy arrays and add a new axis\n",
    "    X_train = np.array(X_train)[:, np.newaxis, :]\n",
    "    X_test = np.array(X_test)[:, np.newaxis, :]\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Dense(256, activation='relu'), input_shape=(1, X_train.shape[2])))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print('Test loss:', loss)\n",
    "    print('Test accuracy:', accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e88bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(audio_file):\n",
    "    features = extract_features(audio_file)\n",
    "    features = features[np.newaxis, np.newaxis, :]  \n",
    "    print(\"Features shape:\", features.shape)\n",
    "    print(\"Features:\", features)\n",
    "\n",
    "    predicted_probabilities = model.predict(features)\n",
    "    print(\"Predicted probabilities shape:\", predicted_probabilities.shape)\n",
    "    print(\"Predicted probabilities:\", predicted_probabilities)\n",
    "\n",
    "    predicted_label_index = np.argmax(predicted_probabilities)\n",
    "    print(\"Predicted label index:\", predicted_label_index)\n",
    "\n",
    "    predicted_emotion = label_encoder.classes_[predicted_label_index]\n",
    "    print(\"Predicted emotion:\", predicted_emotion)\n",
    "\n",
    "\n",
    "    # Emotion mapping for TESS dataset\n",
    "    emotion_mapping = {\n",
    "        'YAF_angry': 'ANGRY',\n",
    "        'YAF_disgust': 'DISGUST',\n",
    "        'YAF_fear': 'FEAR',\n",
    "        'YAF_happy': 'HAPPY',\n",
    "        'YAF_neutral': 'NEUTRAL',\n",
    "        'YAF_pleasant_surprised': 'SURPRISED',\n",
    "        'YAF_sad': 'SAD',\n",
    "        'OAF_angry': 'ANGRY',\n",
    "        'OAF_disgust': 'DISGUST',\n",
    "        'OAF_Fear': 'FEAR',\n",
    "        'OAF_happy': 'HAPPY',\n",
    "        'OAF_neutral': 'NEUTRAL',\n",
    "        'OAF_Pleasant_surprised': 'SURPRISED',\n",
    "        'OAF_Sad': 'SAD',\n",
    "    }\n",
    "\n",
    "\n",
    "    recognizable_emotion = emotion_mapping.get(predicted_emotion)\n",
    "    return recognizable_emotion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac7a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (1, 1, 13)\n",
      "Features: [[[-235.61082     63.643024   -23.374813    -2.4893022  -17.18066\n",
      "    -19.021774   -28.541582   -20.951591   -45.134827   -12.735034\n",
      "    -31.464613    -3.9614031  -17.062605 ]]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "Predicted probabilities shape: (1, 14)\n",
      "Predicted probabilities: [[3.7995379e-03 1.4668463e-03 1.0095182e-06 6.5599215e-01 1.8941424e-05\n",
      "  4.0494226e-04 5.0846770e-06 8.9643185e-04 8.0913730e-04 3.8613048e-03\n",
      "  1.6629406e-03 3.8674239e-05 3.3104244e-01 5.3210607e-07]]\n",
      "Predicted label index: 3\n",
      "Predicted emotion: OAF_angry\n",
      "Features shape: (1, 1, 13)\n",
      "Features: [[[-305.47754     78.43333    -27.72299      7.0501404  -23.963524\n",
      "    -16.117096   -21.372519   -19.396751   -27.249762    -9.337263\n",
      "    -24.362892    -4.109053   -18.191856 ]]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Predicted probabilities shape: (1, 14)\n",
      "Predicted probabilities: [[2.0949723e-04 1.2753595e-04 3.6056147e-07 2.7291818e-02 3.1311743e-06\n",
      "  1.3408395e-04 1.5284477e-06 1.6304678e-03 5.5431789e-03 2.1021077e-03\n",
      "  9.9449614e-03 5.8741029e-05 9.5295018e-01 2.3045159e-06]]\n",
      "Predicted label index: 12\n",
      "Predicted emotion: YAF_pleasant_surprised\n",
      "Features shape: (1, 1, 13)\n",
      "Features: [[[-387.45526     56.905754   -36.03224     -2.7347353  -36.666954\n",
      "    -18.448195   -12.082108    -6.7218165  -12.804758    -7.9460936\n",
      "     -7.1295443    8.550945    -9.683702 ]]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "Predicted probabilities shape: (1, 14)\n",
      "Predicted probabilities: [[3.6060093e-05 4.8605627e-05 8.2608682e-07 1.6298644e-02 4.0202572e-06\n",
      "  2.2889322e-05 5.6270419e-06 8.6386036e-04 1.9834691e-03 1.7331219e-03\n",
      "  6.6435831e-03 1.7310982e-05 9.7234130e-01 6.5843426e-07]]\n",
      "Predicted label index: 12\n",
      "Predicted emotion: YAF_pleasant_surprised\n",
      "Features shape: (1, 1, 13)\n",
      "Features: [[[-342.69534     80.17059    -13.868715    10.016357     2.9826586\n",
      "      8.024366    -0.5431769    3.1576316   -4.3643546   10.758522\n",
      "      8.41267     15.51195      1.2915822]]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Predicted probabilities shape: (1, 14)\n",
      "Predicted probabilities: [[2.3884457e-05 6.0898513e-03 1.0435902e-01 6.4977142e-04 1.1101773e-01\n",
      "  7.2776334e-04 1.1574945e-01 2.3359078e-04 7.7841856e-02 6.7213262e-03\n",
      "  4.5619201e-04 5.6834829e-01 7.3136208e-03 4.6768328e-04]]\n",
      "Predicted label index: 11\n",
      "Predicted emotion: YAF_neutral\n",
      "Features shape: (1, 1, 13)\n",
      "Features: [[[-268.14694     75.065346   -41.623363    20.641575   -26.4011\n",
      "     -8.447602    -5.137443   -10.484287   -21.860046     3.7272005\n",
      "    -21.15929      3.6125777  -13.029993 ]]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "Predicted probabilities shape: (1, 14)\n",
      "Predicted probabilities: [[4.9255759e-05 1.5356185e-04 1.0813440e-05 2.5450235e-04 4.6128353e-05\n",
      "  2.2443375e-05 6.0189195e-06 3.8177755e-03 1.6163529e-03 5.8411777e-01\n",
      "  1.4803648e-03 6.7025109e-04 4.0775040e-01 4.4571893e-06]]\n",
      "Predicted label index: 9\n",
      "Predicted emotion: YAF_fear\n",
      "Features shape: (1, 1, 13)\n",
      "Features: [[[-256.895      96.68024   -46.237297  -13.082202  -36.34462\n",
      "    -34.50653   -21.848871  -23.592604  -41.51541   -14.43111\n",
      "    -33.87978    -6.840948  -27.105797]]]\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "Predicted probabilities shape: (1, 14)\n",
      "Predicted probabilities: [[1.71265288e-04 1.19623794e-04 1.12107415e-07 1.64576564e-02\n",
      "  3.70127782e-06 1.04229912e-04 6.00399574e-07 2.20176624e-03\n",
      "  1.16486649e-03 2.91271787e-03 3.44770425e-03 4.33468595e-05\n",
      "  9.73371625e-01 7.88934017e-07]]\n",
      "Predicted label index: 12\n",
      "Predicted emotion: YAF_pleasant_surprised\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "class EmotionApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Voice Prediction App\")\n",
    "        self.root.configure(bg='skyblue')\n",
    "        self.prediction_history = []  \n",
    "        self.show_home_page()\n",
    "\n",
    "        \n",
    "        \n",
    "    def show_home_page(self):\n",
    "        self.clear_window()\n",
    "        \n",
    "        label = tk.Label(self.root, text=\"Voice Prediction App\", font=('Helvetica bold', 16))\n",
    "        label.pack(pady=20)\n",
    "        \n",
    "        button = tk.Button(self.root, text=\"Audio Prediction\", command=self.show_audio_page, bg='lightblue')\n",
    "        button.pack()\n",
    "        \n",
    "        button_history = tk.Button(self.root, text=\"Prediction History\", command=self.show_history_page, bg='lightgreen')\n",
    "        button_history.pack(pady=10)\n",
    "        \n",
    "        about_button = tk.Button(self.root, text=\"About The App\", command=self.show_about_page, bg='lightblue')\n",
    "        about_button.pack(pady=10)\n",
    "        \n",
    "    def show_audio_page(self):\n",
    "        self.clear_window()\n",
    "        \n",
    "        canvas = tk.Canvas(self.root, width=500, height=500, bg='skyblue')\n",
    "        canvas.pack()\n",
    "        \n",
    "        label1 = tk.Label(self.root, text='Voice Emotion Prediction', font=('Helvetica bold', 26))\n",
    "        canvas.create_window(250, 50, window=label1)\n",
    "        \n",
    "        def upload_audio():\n",
    "            file_path = filedialog.askopenfilename(filetypes=[(\"TESS toronto emotional speech data set\", \"*.wav\")])\n",
    "            if file_path:\n",
    "                predicted_emotion = predict_emotion(file_path)\n",
    "                label2.config(text=predicted_emotion)\n",
    "                \n",
    "                self.prediction_history.append((os.path.basename(file_path), predicted_emotion))\n",
    "                \n",
    "                \n",
    "                \n",
    "        button1 = tk.Button(self.root, text='Upload Audio', command=upload_audio, bg='lightgreen')\n",
    "        canvas.create_window(250, 150, window=button1)\n",
    "        \n",
    "        label2 = tk.Label(self.root, text='Predicted Emotion Will Be Displayed Here')\n",
    "        canvas.create_window(250, 200, window=label2)\n",
    "        \n",
    "       \n",
    "        \n",
    "        back_button = tk.Button(self.root, text=\"Back to Home\", command=self.show_home_page)\n",
    "        canvas.create_window(250, 400, window=back_button)\n",
    "        \n",
    "    def show_history_page(self):\n",
    "        self.clear_window()\n",
    "        \n",
    "        canvas = tk.Canvas(self.root, width=500, height=500, bg='lightgreen')\n",
    "        canvas.pack()\n",
    "        \n",
    "        label = tk.Label(self.root, text=\"Prediction History\", font=('Helvetica bold', 16))\n",
    "        canvas.create_window(250, 50, window=label)\n",
    "        \n",
    "        if self.prediction_history:\n",
    "            for index, (file_name, predicted_emotion) in enumerate(self.prediction_history, start=1):\n",
    "                history_text = f\"{index}. File: {file_name}, Emotion: {predicted_emotion}\"\n",
    "                history_label = tk.Label(self.root, text=history_text)\n",
    "                canvas.create_window(250, 100 + index * 30, window=history_label)\n",
    "        else:\n",
    "            no_history_label = tk.Label(self.root, text=\"No prediction history available.\")\n",
    "            canvas.create_window(250, 150, window=no_history_label)\n",
    "        \n",
    "        back_button = tk.Button(self.root, text=\"Back to Home\", command=self.show_home_page)\n",
    "        canvas.create_window(250, 450, window=back_button)\n",
    "        \n",
    "    def show_about_page(self):\n",
    "        self.clear_window()\n",
    "        \n",
    "        canvas = tk.Canvas(self.root, width=500, height=500, bg='skyblue')\n",
    "        canvas.pack()\n",
    "        \n",
    "        label = tk.Label(self.root, text=\"About The Software\", font=('Helvetica bold', 16))\n",
    "        canvas.create_window(250, 50, window=label)\n",
    "        \n",
    "        about_text = (\"Hello Everyone !! \"\n",
    "                      \" Speech Emotion Recognition is a software that recognizes the emotion of the user.\"\n",
    "                      \" All of the audio files in this software should be inputted with '.wav' extension.\"\n",
    "                     \n",
    "                      )\n",
    "        \n",
    "        about_label = tk.Label(self.root, text=about_text, wraplength=400)\n",
    "        canvas.create_window(250, 150, window=about_label)\n",
    "        \n",
    "        back_button = tk.Button(self.root, text=\"Back to Home\", command=self.show_home_page)\n",
    "        canvas.create_window(250, 400, window=back_button)\n",
    "        \n",
    "    def clear_window(self):\n",
    "        for widget in self.root.winfo_children():\n",
    "            widget.destroy()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = EmotionApp(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51d124-c1ca-4eef-8202-a3e2e58243ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
